kmer2vec model (skipgram).

implementing word2vec algorithm on promotors' sequences 
with sigma70 factor using skip gram model

Hyper-parameters:

embedding_size: 256, learning_rate: 0.001, window_size: 2

ML Data setup is done



similar_words(TGGAAA)

[('TTGACT', 0.9929325580596924), ('TTACGC', 0.9925817251205444), ('ATGTAA', 0.9925590753555298), ('TTGTAA', 0.9925285577774048), ('AGAATA', 0.992506206035614), ('AATCAG', 0.99248868227005), ('TAATGT', 0.9924161434173584), ('AAAGTT', 0.9924044609069824), ('TGTCAT', 0.9923993349075317), ('GCTGAA', 0.9923964738845825)]




number of cpus: 4
kmer size (vocabulary = unique words): 4089
length of data: 4089

training is done!, and model is saved.
